<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content"><channel><title>GrammerJack</title><description>Jack Palevich's Essays</description><link>https://jackpal.github.io</link><language>en</language><lastBuildDate>Sat, 7 Mar 2020 13:21:48 -0800</lastBuildDate><pubDate>Sat, 7 Mar 2020 13:21:48 -0800</pubDate><ttl>250</ttl><atom:link href="https://jackpal.github.io/feed.rss" rel="self" type="application/rss+xml"/><item><guid isPermaLink="true">https://jackpal.github.io/posts/post1</guid><title>Building an image board browser using SwiftUI and Combine</title><description>A description of my first post.</description><link>https://jackpal.github.io/posts/post1</link><pubDate>Sat, 7 Mar 2020 12:03:00 -0800</pubDate><content:encoded><![CDATA[<h1>Building an image board browser using SwiftUI and Combine</h1><p>As a hobby project, I've been writing an imageboard browser app to learn the SwiftUI and Combine libraries.</p><p>SwiftUI and Combine are available on many Apple platforms. So far I've gotten my imageboard browser working well on iPhone and iPad, and this weekend I got it working on Apple TV.</p><p>The iPhone and iPad run the same "Universal" app, which provides a vertical scrolling list and navigation stack UI that works well for both iPhone and iPad:</p><img src="KleeneStariPhone.jpg" alt="iPhone Screenshot"/><img src="KleeneStariPad.jpg" alt="iPad Screenshot"/><p>The AppleTV app looks and acts quite differently:</p><img src="KleeneStarTV.jpg" alt="AppleTV Screenshot"/><p>The AppleTV app has a "lean back" UI. The core feature of the AppleTV app is a slideshow mode. You start the app, use the remote to pick an image board, and then the app plays a slideshow of all the images on that board. There can be thousands of images on some of the more active boards. The slideshow repeats, picking up any updated content each time through.</p><p>I used an elaborate Combine pipeline to fetch the images. This is the heart of the pipeline:</p><p>Combine Pipeline for Image Board Slideshow</p><p>This single animated view displays the slideshow:</p><p>A SwiftUI Slideshow View</p><p>I am using github gists because Blogger doesn't faithfully render code snippets.</p><h1>Issues</h1><p>Although the code looks fairly clean now, I ran into some tricky problems while writing this code.</p><h2>Combine request throttling</h2><p>The slideshow makes multiple nested asynchronous requests to collect the image URLs:</p><p>repeatedly: request all the threads in the image board: for each thread, request all the posts in the thread: for each post, if it is an image post, request the image data: decode the image data into a UIImage I ran into a problem where the earlier steps were not throttled, and so the image data was accumulating at a prodigious rate. I quickly exceeded the 2 GB memory limit for AppleTV apps.</p><p>Interestingly, if I displayed the images as quickly as possible, everything worked fine -- I ran the app for 20 minutes, displaying a new image every quarter of a second. Things only backed up when I changed the pipeline to work like this:</p><p>repeatedly: request all the threads in the image board: for each thread, request all the posts in the thread: for each post, if it is an image post: request the image data: delay for 5 seconds: decode the image data into a UIImage In this pipeline, the image data accumulated in RAM, and that ended up killing the app.</p><p>Now Combine has the concept of back-pressure, and it's supposed to be possible to set up Combine pipelines so that this problem can be avoided. But unfortunately I couldn't get the backpressure to work in this pipeline. It might be due to my limited understanding, but it could also be due to a bug in Combine.</p><p>Luckily, I was able to work-around the issue by changing my pipeline to this:</p><p>repeatedly: request all the threads in the image board: for each thread, request all the posts in the thread: for each post, if it is an image post: delay for 5 seconds: request the image data: decode the image data into a UIImage By moving the "delay for 5 seconds" step before the "request the image data" step, I avoided backing up the pipeline.</p><h2>Image crossfading</h2><p>SwiftUI has an extensive animation system, but it isn't well documented, and it has bugs and limitations. I wasn't able to do exactly what I wanted, but I was at least able to get something working.</p><p>The effect I wanted was: Each time a new image is received, fade it in in front of the previous image. Ideally the opacity of the old image stays at 100%, while the new image's opacity animates from 0% (transparent) to 100% (opaque).</p><p>The effect I was able to achieve was: Each time a new image is received, fade it in in front of the previous image, while simultaneously fading the old image from 100% opacity to 0% opacity.</p><p>This works, but has the drawback that in the middle of the transition, both the old and new images are partially transparent, and so the background color shows through.</p><p>I tried a number of approaches to work around this problem, but I just couldn't get things to work.</p><p>Now that I've got basic crossfading working, I'm tempted to add "Ken Burns" style pan-and-zoom animation, as well as effects like the old AppleTV Photos screensaver.</p><h1>Conclusion</h1><p>For what it's worth, Apple's Instruments tool tells me there's some memory leaks when running the pipeline. These are genuine leaks, rather than retain cycles. The leaked objects appear to be internal to the SwiftUI implementation. Luckily the leaks are on the order of a few kilobytes per second, so it's not too much of a problem in practice.</p><p>I think Combine and SwiftUI are still "beta" quality software. Hopefully things will improve with iOS 14.</p>]]></content:encoded></item></channel></rss>