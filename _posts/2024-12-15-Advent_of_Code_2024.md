---
date: 2024-12-15 12:25:00
description: Report on participating in the 2025 Advent of Code
tags: advent_of_code, gemini, python, networkx, vs_code, jupyter_notebook, advent_of_code_data
title: My 2024 Advent of Code Report
---

This year was the 10th anniversary of Advent of Code. I have been participating since 2020, and I've gone back and done all the previous year's puzzles as well.

This year I participated with my three children. We all used Python. My son, who is 2 years out of college, regularly beat me, typically finishing twice as quickly a me. In analyzing our approaches, it seems that part of the reason is that he uses less abstraction, and he types faster. The combination means that his programs are shorter, and he's able to evolve them more quickly. I still sometimes win when the puzzle solution requires knowing some obscure algorithm, but the difference in our speeds is pretty consistent across multiple days and multiple levels of problem difficulty.

My two daughers (who are still in college) had to drop out of the competition early, due to finals. The were able to solve the early day puzzles fairly quickly, but grew frustated with the more difficult problems.

After the first 12 days, I decided to switch things up. I started using LLMs. In order to avoid polluting the leader board, I waited 10 minutes after the start of each day's contest to begin working on the problem. This kept my LLM-assisteds scores off the leaderboards. (As I understand it, many LLM users were not as polite. Sorry, humans!)

I experimented with the various free models availabel from Google's [AI Studio](https"//aistudio.google.com). In the end, I found that the Gemini 1206 model was the most capable. I didn't find any case where the other models generated better output.

# Prompt Engineering

After experimenting with small tweaks during the contest, my System Instruction prompt was a variation of:

```
You are an expert Python coder. You are participating in the "Advent of Code" programming contest.  The following is a puzzle description. Write expert Python code to solve the puzzle.

The puzzle has two parts. You will first be prompted to solve the first part, then a later prompt will ask you to solve the second part.

First write a short explanation of the algorithm . Then write the code.

Read the puzzle input in the form of a text string from puzzle.input_data. Assign the puzzle answer to the property puzzle.answer_a for the first part. Assign the puzzle answer to puzzle.answer_b for the second part.

Only assign to puzzle.answer_a one time. Don't use puzzle.answer_a as a temporary variable or an accumulator.

For example if the puzzle is, "The input is a series of numbers, one per line. Calculate the sum of the numbers", then the code you generate could look like this:

def solve_a(input_data):
   return sum([int(line) for line in input_data.splitlines()])
puzzle.answer_a = solve_a(puzzle.input_data)

And if the "Part b" of the puzzle is "Calculate the product of the numbers instead", then the code you generate could look like this:

def solve_b(input_data):
  return prod([int(line) for line in input_data.splitlines()])
puzzle.answer_b = solve_b(puzzle.input_data)

Assume that the input is valid. Do not validate the input.

Think carefully. It is important to get the correct answer and for the program to run quickly.

Use split('\n\n') to split chunks of the input that are separated by blank lines.

Pay attention to whitespace in the input. You may need to use "strip()" to remove whitespace.

Use @cache for recursive functions.

If the problem is a graph or network problem, use the networkx library to solve it.

Use subroutines, lambdas, list comprehensions and logical boolean operators where it will make the code shorter.

Use short function names and short variable names.

Assume this code has already been executed to get the puzzle object set up:

    from aocd.models import Puzzle
    from pathlib import Path
    puzzle = Puzzle(year=2024, day=int(Path(__vsc_ipynb_file__).stem))
```

Gemini did not always adhere to this prompt, but it usually did.

A tip for competition is that it takes Gemini significantly more time to generate code with long variable names and comments than it does to generate short code without comments. For Advent of Code puzzles it's often clearer to use short variable names,  so that more of the algorithm can fit on screen.

I started using Gemini around day 14. After that, I went back and tried solving the earlier problems as well. Here are my day-by-day notes on how successfull I was. I score the LLM performance on a scale:

Rating | Meaning
------ | -------
0 | LLM did not produce a working solution, even when prompted with a hint that explained how to solve the problem.
1 | LLM produced a working solution after being prompted with a hint that explained how to solve the problem.
2-3 | LLM required help to debug mistakes in its code (such as off-by-one errors or coordinate system errors.) Score a 2 if the mistake is difficult to debug, 3 if the mistake is easy to debug.
4 | LLM produced a working solution after being prompted with simple prompts like "make it run faster".
5 | LLM produced a working solution, no additional prompts required.

Gemini 1206 performance

Day | Part 1 | Part 2 | Notes
--: | -----: | -----: | -----
1 | 5 | 5 |
2 | 5 | 5 |
3 | 5 | 5 |
4 | 5 | 5 | Some versions of system prompt had problems generating code for part 2.
5 | 5 | 5 |
6 | 5 | 2 | Faulty "guard against infinite loop code" in part 2.
7 | 5 | 5 |
8 | 5 | 5 |
9 | 5 | 5 |
10 | 5 | 5 |
11 | 5 | 2 | That's still too slow. Can you think of a different way of counting the stones?
12 | 5 | 1 | Had trouble counting edges correctly. Had to be prompted with corner-counting algorithm.
13 | 5 | 1 | Use Cramer's Rule
14 | 5 | 1 | See [Day 14 Easter egg prompt](#Day-14-Easter-egg-prompt) below
15 | 1 | 0 | Part 1 prompt: You have to recursively check-and-move the boxes. No known working prompt chain for part 2.
16 | 5 | 1 | See [Day 16 part 2 prompt](#Day-16-part-2-prompt)
17 | 2 | 0 | See [Day 17 part 2](#Day-17-part-2) for details.
18 | 5 | 5 |
19 | 3 | 3 | [Mistakes parsing input](#Day-19-whitespace-parsing).
20 | 1 | 1 | Succeeds with [Reddit prompt](#day-20-race-condition) 
21 | 0 | 0 | Unable to code Reddit hints correctly
22 | 5 | 2 | Part 2 required prompts to optimize runtime and manual debugging of generated code.
23 | 5 | 5 | Part 1 and 2 both used networkx algorithms
24 | | |
25 | | n/a |


# Interesting Days

## Day 14 Easter egg prompt

Day 14 was an unusual Advent of Code Puzzle. The part 2 directions did not provide a way of detecting a Christmas tree. The LLM was completely lost.

Once this hint was provided, the LLM produced code that worked well:

```
That's not correct. Run the simulation normally, but each step, calculate the number of horizontally or vertically adjacent robots. Two robots are horizontally adjacent if their x coordinates differ by 1 and their y coordinates are equal. Two robots are vertically adjacent if their y coordinates differ by 1 and their x coordinates are equal. Keep track of the maximum number of adjacent robots seen. Simulate 10000 steps and then print the step number that had the maximum number of adjacent robots.
```

I'm using the agreed upon scoring rubric, but it could be argued that this response should be scored more highly, because the puzzle directions were intentionally obscure.

## Day 15 part 2

Nothing worked. Lots of incorrect code generated. The LLM had trouble maintaining the data structures for the wide-box simulation, even when prompted with the specific answers.

## Day 16 part 2 prompt

Used this prompt, which is a lightly edited version of a [Reddit comment](https://www.reddit.com/r/adventofcode/comments/1hfboft/comment/m2bae5n)

```
That's not correct. Here is a hint: Run a Dijkstra to get the distance matrix (from_start).

Obtain from_end by running another Dijkstra, but with 4 starting vertices [ (target_row, target_col, dir) for dir in "EWNS"]

Iter through every coord and direction and check if from_start[(row,col,dir)] + from_end[(row,col,flip(dir)]) is the same as answer from part 1.
```

## Day 17 part 2

To solve day 17 part 2 you had to analyze your specific input program, reverse engineer it, and then write a search function that searched a narrow, deep, tree.

The LLM made mistakes on both part 1 and part. It was unable to provide a useful program for part 2. But it was able to generate correct subroutines when directed to by special-purpose prompts. For example I asked it to disassemble my puzzle input, turning it into Python code, and it did a fairly good job, making just one mistake. Similarly, when I asked it to write a search function, it wrote one quickly.

## Day 19 whitespace parsing

Both parts had problems parsing the puzzle input. Items were separated by
', ', but the LLM split on ',' instead of the correct ', '.

## Day 20 Race Condition

No luck with LLM or on my own. LLM solved easily with this Reddit-provided hint:

```
Both parts 1 and 2 can be solved using the same algorithm. Write a single function solve(allowed_cheat_length) that is used for both solutions.

We find the distance from end position to any other position.

We find the path from start to end using knowledge of the distance from any point to end

For each pair (a,b) where a and b are some coordinates of the fastest path

calculate Manhattan distance between a and b

if this distance larger than allowed cheat, a -> b is not an allowed cheat

if the distance from b to the end + distance from a to b is not lower than the distance from a to b, then there is no benefit of such "cheat"

if the benefit of the move from a to b is less than defined minimal benefit - we ignore it

otherwise, increase counter of valid cheats by 1
```
# Uses beyond solving puzzles

Gemini 1206 is good for more than just trying to solve the puzzle:

+ Parsing. Gemini does a pretty good job of parsing puzzle input. I did run into two flaws:
  + Gemini needs help parsing input that uses blank lines to separate different chunks of input. I had to explicitly instruct it, "Use split('\n\n') to parse chunks separated by blank lines.

  + Gemini is blind to whitepace following commas, so it doesn't parse "a, b" correctly. It uses split(',') instead of split(', '). I wasn't able to discover a prompt to fix this. The best I figured out was to either manually correct the issue, or to ask it to strip() the resulting parts.

+ Debugging. I was stuck helping my friend debug their code for Day 16 Part 1. His code solved the examples, but not his input. We worked on the code for 30 minutes, rewriting almost all of it. But nothing helped. Finally, we dumped his original code into Gemini and aske Gemini to find the bug. It immediately pointed out the bug (a direction table with a copy-and-paste error).

+ Visualization. Several days I asked Gemini to add a visualization to my code. (For example, showing a path on a grid.) It was simple code, but Gemini wrote it more quickly than I could have.

# Comparing different versions of Gemini

During Advent of Code 2024 I played with several different versions of Gemini

+ Gemini 2.0 Flash Experimental
+ Gemini Experimental 1206
+ Gemini 2.0 Flash Thinking Experimental

As far as I could tell, Gemini Experimental 1206 gave the best results for Advent of Code. I did not find any case where either of the other two models gave a better result than Gemini Experimental 1206 on a given problem.

